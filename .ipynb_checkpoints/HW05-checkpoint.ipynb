{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3e8c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5\n",
      "Number of unique input tokens: 47\n",
      "Number of unique output tokens: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 00:36:40.683724: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.6000 - val_loss: 4.5446\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 2.5743 - val_loss: 4.5540\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 2.5414 - val_loss: 5.3264\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 2.2317 - val_loss: 5.1601\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 2.1782 - val_loss: 5.3688\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 2.2639 - val_loss: 5.5029\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 2.1728 - val_loss: 5.5554\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 2.0699 - val_loss: 5.6604\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.0261 - val_loss: 5.6889\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.9860 - val_loss: 5.7857\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 2.0016 - val_loss: 5.7587\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.9856 - val_loss: 5.7172\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 2.0752 - val_loss: 5.8313\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.9445 - val_loss: 5.7675\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.9436 - val_loss: 5.8195\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.9020 - val_loss: 5.7209\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 2.0288 - val_loss: 5.7996\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 1.9160 - val_loss: 5.8069\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.9160 - val_loss: 5.8415\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.8789 - val_loss: 5.7441\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 2.0336 - val_loss: 5.8368\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 1.8935 - val_loss: 5.8488\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.8508 - val_loss: 5.9133\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 1.8161 - val_loss: 5.8161\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 2.0092 - val_loss: 5.9309\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.9873 - val_loss: 5.9754\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 1.9367 - val_loss: 5.9281\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2.1130 - val_loss: 6.0550\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 2.0561 - val_loss: 5.9781\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 2.0400 - val_loss: 6.1141\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 2.0036 - val_loss: 6.0077\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 2.0037 - val_loss: 6.1019\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.9689 - val_loss: 6.0195\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.9818 - val_loss: 6.1642\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.9572 - val_loss: 5.9252\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 2.0175 - val_loss: 6.0134\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.9279 - val_loss: 6.0947\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 1.8887 - val_loss: 6.1302\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.8796 - val_loss: 6.0765\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.9160 - val_loss: 6.1701\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.9000 - val_loss: 5.9844\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.9888 - val_loss: 6.0903\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.8438 - val_loss: 6.1587\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.8218 - val_loss: 6.1253\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.8269 - val_loss: 6.2325\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.8750 - val_loss: 5.9036\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 2.0773 - val_loss: 5.9613\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.9464 - val_loss: 6.0506\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.8773 - val_loss: 6.1109\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.8261 - val_loss: 6.1346\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.7956 - val_loss: 6.1572\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.7848 - val_loss: 6.2552\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.9207 - val_loss: 6.0439\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.9539 - val_loss: 6.0370\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.8218 - val_loss: 6.1889\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.7484 - val_loss: 6.2143\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.8155 - val_loss: 5.9555\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 2.0779 - val_loss: 6.0380\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.9483 - val_loss: 6.0838\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.8525 - val_loss: 6.1106\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.8581 - val_loss: 6.0610\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.8328 - val_loss: 6.0442\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.8573 - val_loss: 6.0008\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.9012 - val_loss: 6.0200\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.7710 - val_loss: 6.0637\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 1.7981 - val_loss: 6.0511\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.8384 - val_loss: 6.1283\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.7428 - val_loss: 6.1438\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.7630 - val_loss: 6.2116\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.7170 - val_loss: 6.2441\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.7416 - val_loss: 6.2157\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.7250 - val_loss: 6.1828\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.6436 - val_loss: 6.2173\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.6126 - val_loss: 6.2225\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.5945 - val_loss: 6.2773\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 1.6158 - val_loss: 6.1328\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.9244 - val_loss: 6.0392\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.7924 - val_loss: 6.0266\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.8519 - val_loss: 6.0344\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.7302 - val_loss: 6.0350\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.9284 - val_loss: 6.0967\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.6932 - val_loss: 6.2297\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step - loss: 1.6557 - val_loss: 6.0464\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.7927 - val_loss: 6.0653\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.6430 - val_loss: 6.1392\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.6029 - val_loss: 6.1897\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.5937 - val_loss: 6.1880\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.7546 - val_loss: 6.1708\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 1.5605 - val_loss: 6.2188\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.5227 - val_loss: 6.2669\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.4936 - val_loss: 6.1688\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.5389 - val_loss: 6.2413\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.4669 - val_loss: 6.2341\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.6719 - val_loss: 6.2166\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.5104 - val_loss: 6.2729\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.5012 - val_loss: 6.2733\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.5454 - val_loss: 6.1475\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.4195 - val_loss: 6.2386\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.4263 - val_loss: 6.1970\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 1.3751 - val_loss: 6.2727\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "# num_samples = 5200000  # Number of samples to train on.\n",
    "num_samples = 5\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "def load_data():\n",
    "    with open('translation2019zh/translation2019zh_train.json', \"r\", encoding = \"utf-8\") as f:\n",
    "        data = f.readlines()[0:5]\n",
    "        temp = []\n",
    "        for d in data:\n",
    "            temp = json.loads(d)\n",
    "            input_text = temp['english']\n",
    "            target_text = temp['chinese']\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "            for char in input_text:\n",
    "                if char not in input_characters:\n",
    "                    input_characters.add(char)\n",
    "            for char in target_text:\n",
    "                if char not in target_characters:\n",
    "                    target_characters.add(char)\n",
    "def setting():\n",
    "    input_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(input_characters)])\n",
    "    target_token_index = dict(\n",
    "        [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        for t, char in enumerate(target_text):\n",
    "            decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "            if t > 0:\n",
    "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    \n",
    "\n",
    "def training():\n",
    "    #encoder\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    #decoder\n",
    "    decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    #model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2)\n",
    "    model.save('s2s.h5')\n",
    "\n",
    "def define_model():\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Reverse-lookup token index to decode sequences back to\n",
    "    # something readable.\n",
    "    reverse_input_char_index = dict(\n",
    "        (i, char) for char, i in input_token_index.items())\n",
    "    reverse_target_char_index = dict(\n",
    "        (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "load_data()\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "# max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "# max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "# print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "setting()\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba58ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
